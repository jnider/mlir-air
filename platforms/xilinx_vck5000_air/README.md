# VCK5000 AIR platform (Vitis 2022.1)

This platform is designed to provide a configuration for VCK5000 PCIe, x86 hosted cards for the purpose of supporting a datacenter environement to test mlir generated designs for AI Engines. The build flow is compatible with AMD-Xilinx VCK5000 cards with production silicon: "VCK5000-AIE-ADK-G-ED". It is built with 2022.1 Vivado/Vitis tools and consists of a relatively empty Versal design (CIPS, QDMA-PCIe frontend, BRAM 'Queue Memory', NoC configuration, and CDMA 'AIE-Configuration DMA'). The ARM processor acts as an HSA AQL packet processor that manages AIE configurations and affects runtime DMA transfers from external memory into the AIE array. The build process consists of 2 steps: vivado, aie_platform:

1. Vivado IPI design. This step generates a Vitis extensible hardware platform containing the necessary CIPS configuration and hardware IP components. This step generates an .xsa file for the design.
2. Vitis design compiles a 32 GMIO AIE design with simple add functionality to enable all NoC NMU/NSU connections so all shimDMAs (used for GMIO) are enabled. This rebuilds the design targeting the platform generated by step 1.

## Prerequisites
Vivado 2022.1
Vitis 2022.1
VCK5000 board files\*: `vck5000_board_files_prod_si_20211201.zip`

\* The board files can be downloaded from the [VCK5000 Early Access Site](https://www.xilinx.com/member/vck5000.html). Note that the xsa build script (vivado/xilinx_vck5000_air_xsa.tcl) assumes the board files are stored in mlir-air/platforms/other-board-files/vck5000. Users can override this path by creating and setting an environment variable with the name `MLIR_AIR_OTHER_BOARD_FILE_DIR`.

The pdi is loaded to the card over JTAG, the USB-JTAG cable must be connected to the micro-USB input on the VCK5000 card and a programming machine (this can be the x86 host). The Xilinx Cable drivers must be [installed](https://docs.xilinx.com/r/en-US/ug973-vivado-release-notes-install-license/Installing-Cable-Drivers) on the programming machine.

## Build steps

The first step is to build the BlackParrot IP and Firmware. See the READMEs and Makefiles in the following directories to prepare the BlackParrot IP block and its firmware.

    mlir-air/platforms/ip/black-parrot
    mlir-air/runtime_lib/controller

The `blackparrot_ip.tar.gz` generated from the BlackParrot IP flow should be extracted in the `vivado/` directory. The `main_vivado.mem` from the BlackParrot firmware flow should be copied as `main.mem` to the `vivado/` directory. The BlackParrot IP and firmware only needs to be rebuilt if either changes. They do not need to be rebuilt when rebuilding the platform after making platform-only changes.

To build the .pdi of this design that can be loaded to the card, simply call:
```
make all
```

## Programming steps
Once run the top-level make completes, you should have generated aie_platform/final_vck5000.pdi containing the VCK5000 platform. The pdi can be loaded to the card by calling:
```
cd aie_platform
make program_vck5000
```
After programming the host should undergo a **warm reboot**.
NOTE: the machine hosting the VCK5000 card will most likely crash after programming the card. This is because the PCIe link is lost during reconfiguration and the host may report an error. This is normal, and the card will be reenumerated on the PCIe bus after a **warm reboot**.

## Verification
After a warm reboot, you can verify that the card has been programmed properly with the VCK5000 AIR platfrorm by executing this command:
```
sudo lspci -vd 10ee:
```
The output should match the following (perhaps with a different bus ID):
```
21:00.0 Memory controller: Xilinx Corporation Device b034
        Subsystem: Xilinx Corporation Device 0007
        Flags: bus master, fast devsel, latency 0, IRQ 5, NUMA node 0
        Memory at c0000000 (64-bit, non-prefetchable) [size=64M]
        Memory at a0000000 (64-bit, non-prefetchable) [size=512M]
        Memory at c4000000 (64-bit, non-prefetchable) [size=2M]
        Capabilities: [40] Power Management version 3
        Capabilities: [70] Express Endpoint, MSI 00
        Capabilities: [100] Advanced Error Reporting
        Capabilities: [1c0] Secondary PCI Express
        Capabilities: [1f0] Virtual Channel
```

## Scale-out

The VCK5000 AIR platform utilizes two [Embedded RDMA Enabled NICs (ERNICs)](https://www.xilinx.com/products/intellectual-property/ef-di-ernic.html) which each provide 100G RoCEv2 scale-out networking interfaces over the two QSFP cages of the VCK5000. The ARM processor manages the two ERNICs by posting RDMA operations to the ERNIC. More details of the RDMA-oriented HSA packets can be found in the ARM source code as well and the tests that utilize the scale-out functionality. Tests that utilize the VCK5000 Scale-out capability are located in mlir-air/test and numbered in the 300s and are named with a scale-out or rdma identifier. We also provide a standalone test in mlir-air/runtime_lib/test/7_pcie_ernic_mrmac_standalone which bypasses the ARM and AIEs, and has two userpsace programs directly post RDMA commands to the ERNICs. This can be useful to test and debug network connectivity and platform functionality prior to incorporating the rest of AIR into your design. Each scale-out test will have documentation of how the cards should be connected and how the tests should be run.

## Driver
After programming the card, rebooting the host, and verifying the the card has been programmed properly, the [AIR PCIe driver](https://github.com/Xilinx/mlir-air/tree/main/driver) must be loaded to communicate with the card.

-----

<p align="center">Copyright&copy; 2019-2022 Advanced Micro Devices, Inc.</p>
